{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1542260604168,
     "user": {
      "displayName": "박준우",
      "photoUrl": "",
      "userId": "09139914674254720430"
     },
     "user_tz": -540
    },
    "id": "WC7bv8zXvXK6",
    "outputId": "92410386-ac16-4207-d59b-52236c06e1b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n"
     ]
    }
   ],
   "source": [
    "f = open('./자가증식텍스트.txt', encoding='utf-8')\n",
    "\n",
    "sentences = list()\n",
    "golden = list()\n",
    "for line in f.read().splitlines():\n",
    "    sentences.append(line.split()[0])\n",
    "    golden.append(int(line.split()[1]))\n",
    "f.close()\n",
    "\n",
    "f = open('./yonsei_sendData.txt', encoding='utf-8')\n",
    "\n",
    "origin_sentences = list()\n",
    "origin_tag = list()\n",
    "\n",
    "for line in f.read().splitlines():\n",
    "    origin_sentences.append(line.split(',')[2].replace(' ',''))\n",
    "    if line.split(',')[0] == '1.0':\n",
    "        origin_tag.append(1)\n",
    "    else:\n",
    "        origin_tag.append(0)\n",
    "f.close()\n",
    "\n",
    "print(len(origin_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yZmbWn7TqzhF"
   },
   "outputs": [],
   "source": [
    "def get_score(predict):\n",
    "    tp,tn,fp,fn = 0,0,0,0\n",
    "    for i in range(len(golden)):\n",
    "        if golden[i] == 1 and predict[i] == 1:\n",
    "            tp += 1\n",
    "        elif golden[i] == 1 and predict[i] == 0:\n",
    "            fn += 1\n",
    "        elif golden[i] == 0 and predict[i] == 0:\n",
    "            tn += 1\n",
    "        elif golden[i] == 0 and predict[i] == 1:\n",
    "            fp += 1\n",
    "    acc = float(tp+tn)/len(golden)        \n",
    "    precision = float(tp)/(tp+fp)\n",
    "    recall = float(tp)/(tp+fn)\n",
    "    fscore = (precision*recall)/(precision+recall)*2\n",
    "    return fscore, precision, recall, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1507,
     "status": "ok",
     "timestamp": 1542260609812,
     "user": {
      "displayName": "박준우",
      "photoUrl": "",
      "userId": "09139914674254720430"
     },
     "user_tz": -540
    },
    "id": "9gsrBqLuveTq",
    "outputId": "73488b8b-7d7e-49c5-e196-b415de96f137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7133757961783439 0.7 0.7272727272727273 0.85\n"
     ]
    }
   ],
   "source": [
    "f = open('./blacklist1.txt',encoding='utf-8')\n",
    "\n",
    "black_list = list()\n",
    "for line in f.read().splitlines():\n",
    "    black_list.append(line.split()[0])\n",
    "f.close()\n",
    "\n",
    "black_list = list(set(black_list))\n",
    "predict_blacklist = list()\n",
    "for sentence in sentences:\n",
    "    black_flag = 0\n",
    "    for black_word in black_list:\n",
    "        if black_word in sentence:\n",
    "            black_flag = 1\n",
    "            break\n",
    "    if black_flag == 1:\n",
    "        predict_blacklist.append(1)\n",
    "    else:\n",
    "        predict_blacklist.append(0)\n",
    "fscore1,a,b,c = get_score(predict_blacklist)\n",
    "print(fscore1,a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cSltO3FPhWFS"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "LFGS0k5khQQW"
   },
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fNLzR_YihQQZ"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "UNK_token = 2\n",
    "\n",
    "MAX_LENGTH = 34\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\"}\n",
    "        self.n_words = 3 # Count SOS and EOS\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "input_lang = Lang('src')\n",
    "output_lang = Lang('targ')\n",
    "\n",
    "# src_f = open('./data/original_src-train.txt','r')\n",
    "# targ_f = open('./data/original_targ-train.txt','r')\n",
    "\n",
    "src_f = open('./non비속어_balanced.txt','r',encoding='UTF8')\n",
    "# targ_f = open('./original_targ-train.txt','r')\n",
    "\n",
    "src, targ = list(),list()\n",
    "\n",
    "for line in src_f.read().splitlines():\n",
    "    src_element = line.split('\\t')[0].replace(\" \", \"\")\n",
    "    if len(src_element) > 20:\n",
    "        src_element = src_element[len(src_element)-20:len(src_element)]\n",
    "        src_element = list(src_element)\n",
    "        src_element = ' '.join(src_element)\n",
    "        src.append(src_element)\n",
    "    else:\n",
    "        src_element = list(src_element)\n",
    "        src_element = ' '.join(src_element)\n",
    "        src.append(src_element)\n",
    "    targ.append(line.split('\\t')[1])\n",
    "    input_lang.index_words(src_element)\n",
    "    output_lang.index_words(line.split('\\t')[1])\n",
    "    \n",
    "pairs = list()\n",
    "for i in range(0,len(src)):\n",
    "    pairs.append([src[i]] + [targ[i]])\n",
    "\n",
    "tmp_dict = {}\n",
    "\n",
    "for element in input_lang.word2count.items():\n",
    "  if element[1] > 1:\n",
    "    tmp_dict[element[0]] = element[1]\n",
    "\n",
    "input_lang.word2count = tmp_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hMBq4y98hQQh"
   },
   "outputs": [],
   "source": [
    "def indexes_from_sentence(lang, sentence):\n",
    "    indexes = list()\n",
    "    for word in sentence.split(' '):\n",
    "      if word in lang.word2count:\n",
    "        indexes.append(lang.word2index[word])\n",
    "      else:\n",
    "        indexes.append(2)\n",
    "    return indexes\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)  \n",
    "    indexes.append(EOS_token)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1,1))\n",
    "#   print('var = ',var)\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(pair):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kPDtjlcJhQQk"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bbQ_XJd-hQQn"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = GeneralAttn(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note that we will only be running forward for a single decoder time step, but will use all encoder outputs\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "21uhTM-IhQQq"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size, max_length=MAX_LENGTH):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(seq_len)) # B x 1 x S\n",
    "        if USE_CUDA: attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
    "        return F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.other.dot(energy)\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1xO2dkxXhQQt"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep parameters for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "\n",
    "        # Combine embedded input word and last context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "\n",
    "        # Final output layer (next word prediction) using the RNN hidden state and context vector\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)))\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2siI1p2ZhQQ9"
   },
   "outputs": [],
   "source": [
    "#load_model\n",
    "encoder = torch.load('encoder_general_100000.pth')\n",
    "decoder = torch.load('decoder_general_100000.pth')\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 897,
     "status": "ok",
     "timestamp": 1541983054666,
     "user": {
      "displayName": "박준우",
      "photoUrl": "",
      "userId": "09139914674254720430"
     },
     "user_tz": -540
    },
    "id": "IafTdchwjhSL",
    "outputId": "17845340-5187-4992-e885-8009caba03e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['없 는 생 색 다 내 는 꼬 락 서 니 보 느 니 내 가 갈 고 만 다', '2']"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wli3HuqnhQRK"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = variable_from_sentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]])) # SOS\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "#             decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4sjWcMHlhQRM"
   },
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    \n",
    "    output_words, decoder_attn = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1541939433933,
     "user": {
      "displayName": "박준우",
      "photoUrl": "",
      "userId": "09139914674254720430"
     },
     "user_tz": -540
    },
    "id": "YquwhKINbtgB",
    "outputId": "e7773e65-8d51-4d0f-8a8e-3df52087f363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "output_words, decoder_attn = evaluate('의사의 꿈 키워 힘들게 공부하고 의대 진학해 의사 된 장군이.. 집안 형편도 어렵고 몸도 안 좋아 공부에도 한계가 있어 지방대 의대 감')\n",
    "output_sentence = ' '.join(output_words)\n",
    "print(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rQaTQFxihQRQ"
   },
   "outputs": [],
   "source": [
    "evaluate_randomly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5367
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12755,
     "status": "ok",
     "timestamp": 1542263575825,
     "user": {
      "displayName": "박준우",
      "photoUrl": "",
      "userId": "09139914674254720430"
     },
     "user_tz": -540
    },
    "id": "HyOAK0kyhQRT",
    "outputId": "11e92443-4dff-45d5-8927-1271b69b21b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/Colab Notebooks/verify_ethics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 2\n",
      "1 : 2\n",
      "2 : 1\n",
      "3 : 1\n",
      "4 : 3\n",
      "5 : 3\n",
      "6 : 3\n",
      "7 : 2\n",
      "8 : 1\n",
      "9 : 2\n",
      "10 : 1\n",
      "11 : 1\n",
      "12 : 3\n",
      "13 : 2\n",
      "14 : 3\n",
      "15 : 2\n",
      "16 : 3\n",
      "17 : 1\n",
      "18 : 1\n",
      "19 : 3\n",
      "20 : 3\n",
      "21 : 2\n",
      "22 : 3\n",
      "23 : 1\n",
      "24 : 2\n",
      "25 : 3\n",
      "26 : 2\n",
      "27 : 1\n",
      "28 : 1\n",
      "29 : 3\n",
      "30 : 3\n",
      "31 : 3\n",
      "32 : 3\n",
      "33 : 1\n",
      "34 : 3\n",
      "35 : 3\n",
      "36 : 2\n",
      "37 : 2\n",
      "38 : 3\n",
      "39 : 2\n",
      "40 : 3\n",
      "41 : 1\n",
      "42 : 1\n",
      "43 : 2\n",
      "44 : 1\n",
      "45 : 2\n",
      "46 : 3\n",
      "47 : 3\n",
      "48 : 3\n",
      "49 : 3\n",
      "50 : 1\n",
      "51 : 3\n",
      "52 : 3\n",
      "53 : 3\n",
      "54 : 3\n",
      "55 : 1\n",
      "56 : 1\n",
      "57 : 2\n",
      "58 : 3\n",
      "59 : 3\n",
      "60 : 1\n",
      "61 : 2\n",
      "62 : 3\n",
      "63 : 3\n",
      "64 : 3\n",
      "65 : 3\n",
      "66 : 2\n",
      "67 : 3\n",
      "68 : 2\n",
      "69 : 3\n",
      "70 : 3\n",
      "71 : 3\n",
      "72 : 1\n",
      "73 : 1\n",
      "74 : 3\n",
      "75 : 1\n",
      "76 : 1\n",
      "77 : 1\n",
      "78 : 3\n",
      "79 : 1\n",
      "80 : 3\n",
      "81 : 3\n",
      "82 : 1\n",
      "83 : 3\n",
      "84 : 1\n",
      "85 : 3\n",
      "86 : 3\n",
      "87 : 3\n",
      "88 : 2\n",
      "89 : 2\n",
      "90 : 3\n",
      "91 : 1\n",
      "92 : 3\n",
      "93 : 2\n",
      "94 : 3\n",
      "95 : 3\n",
      "96 : 1\n",
      "97 : 3\n",
      "98 : 3\n",
      "99 : 3\n",
      "100 : 1\n",
      "101 : 3\n",
      "102 : 1\n",
      "103 : 1\n",
      "104 : 3\n",
      "105 : 1\n",
      "106 : 1\n",
      "107 : 3\n",
      "108 : 1\n",
      "109 : 3\n",
      "110 : 3\n",
      "111 : 1\n",
      "112 : 3\n",
      "113 : 3\n",
      "114 : 3\n",
      "115 : 1\n",
      "116 : 2\n",
      "117 : 2\n",
      "118 : 1\n",
      "119 : 3\n",
      "120 : 2\n",
      "121 : 3\n",
      "122 : 2\n",
      "123 : 3\n",
      "124 : 1\n",
      "125 : 2\n",
      "126 : 3\n",
      "127 : 2\n",
      "128 : 3\n",
      "129 : 3\n",
      "130 : 3\n",
      "131 : 1\n",
      "132 : 3\n",
      "133 : 1\n",
      "134 : 3\n",
      "135 : 2\n",
      "136 : 1\n",
      "137 : 3\n",
      "138 : 3\n",
      "139 : 3\n",
      "140 : 1\n",
      "141 : 1\n",
      "142 : 2\n",
      "143 : 1\n",
      "144 : 2\n",
      "145 : 1\n",
      "146 : 3\n",
      "147 : 2\n",
      "148 : 3\n",
      "149 : 1\n",
      "150 : 3\n",
      "151 : 3\n",
      "152 : 1\n",
      "153 : 1\n",
      "154 : 2\n",
      "155 : 3\n",
      "156 : 1\n",
      "157 : 3\n",
      "158 : 3\n",
      "159 : 1\n",
      "160 : 3\n",
      "161 : 2\n",
      "162 : 3\n",
      "163 : 2\n",
      "164 : 3\n",
      "165 : 1\n",
      "166 : 1\n",
      "167 : 2\n",
      "168 : 2\n",
      "169 : 1\n",
      "170 : 3\n",
      "171 : 3\n",
      "172 : 3\n",
      "173 : 3\n",
      "174 : 3\n",
      "175 : 1\n",
      "176 : 3\n",
      "177 : 1\n",
      "178 : 3\n",
      "179 : 2\n",
      "180 : 3\n",
      "181 : 2\n",
      "182 : 1\n",
      "183 : 1\n",
      "184 : 3\n",
      "185 : 1\n",
      "186 : 3\n",
      "187 : 3\n",
      "188 : 3\n",
      "189 : 3\n",
      "190 : 2\n",
      "191 : 3\n",
      "192 : 3\n",
      "193 : 2\n",
      "194 : 2\n",
      "195 : 3\n",
      "196 : 3\n",
      "197 : 2\n",
      "198 : 2\n",
      "199 : 1\n",
      "200 : 2\n",
      "201 : 1\n",
      "202 : 1\n",
      "203 : 3\n",
      "204 : 3\n",
      "205 : 3\n",
      "206 : 3\n",
      "207 : 2\n",
      "208 : 3\n",
      "209 : 2\n",
      "210 : 3\n",
      "211 : 3\n",
      "212 : 3\n",
      "213 : 1\n",
      "214 : 1\n",
      "215 : 1\n",
      "216 : 1\n",
      "217 : 3\n",
      "218 : 1\n",
      "219 : 3\n",
      "220 : 1\n",
      "221 : 3\n",
      "222 : 1\n",
      "223 : 1\n",
      "224 : 3\n",
      "225 : 3\n",
      "226 : 3\n",
      "227 : 2\n",
      "228 : 3\n",
      "229 : 1\n",
      "230 : 1\n",
      "231 : 3\n",
      "232 : 1\n",
      "233 : 1\n",
      "234 : 3\n",
      "235 : 3\n",
      "236 : 3\n",
      "237 : 1\n",
      "238 : 2\n",
      "239 : 3\n",
      "240 : 3\n",
      "241 : 1\n",
      "242 : 3\n",
      "243 : 3\n",
      "244 : 2\n",
      "245 : 3\n",
      "246 : 1\n",
      "247 : 2\n",
      "248 : 2\n",
      "249 : 3\n",
      "250 : 1\n",
      "251 : 1\n",
      "252 : 3\n",
      "253 : 2\n",
      "254 : 3\n",
      "255 : 3\n",
      "256 : 1\n",
      "257 : 3\n",
      "258 : 3\n",
      "259 : 1\n",
      "260 : 3\n",
      "261 : 1\n",
      "262 : 3\n",
      "263 : 1\n",
      "264 : 1\n",
      "265 : 2\n",
      "266 : 1\n",
      "267 : 3\n",
      "268 : 2\n",
      "269 : 3\n",
      "270 : 1\n",
      "271 : 1\n",
      "272 : 2\n",
      "273 : 2\n",
      "274 : 3\n",
      "275 : 1\n",
      "276 : 1\n",
      "277 : 3\n",
      "278 : 1\n",
      "279 : 3\n",
      "280 : 2\n",
      "281 : 3\n",
      "282 : 3\n",
      "283 : 2\n",
      "284 : 3\n",
      "285 : 3\n",
      "286 : 3\n",
      "287 : 3\n",
      "288 : 3\n",
      "289 : 1\n",
      "290 : 3\n",
      "291 : 1\n",
      "292 : 2\n",
      "293 : 3\n",
      "294 : 3\n",
      "295 : 1\n",
      "296 : 3\n",
      "297 : 2\n",
      "298 : 3\n",
      "299 : 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_src_f = open('./non비속어_test_balanced.txt','r')\n",
    "try:\n",
    "    f = open('./result.txt','r')\n",
    "    f.close()\n",
    "except:\n",
    "    test_src_f = open('./자가증식텍스트.txt','r')\n",
    "    # test_targ_f = open('./data/data_50000/targ-test.txt','r')\n",
    "\n",
    "    test_src = list()\n",
    "    test_targ = list()\n",
    "    test_pair = list()\n",
    "    ##\n",
    "    for line in src_f.read().splitlines():\n",
    "        src_element = line.split('\\t')[0].replace(\" \", \"\")\n",
    "        if len(src_element) > 20:\n",
    "            src_element = src_element[len(src_element)-20:len(src_element)]\n",
    "            src_element = list(src_element)\n",
    "            src_element = ' '.join(src_element)\n",
    "            src.append(src_element)\n",
    "        else:\n",
    "            src_element = list(src_element)\n",
    "            src_element = ' '.join(src_element)\n",
    "            src.append(src_element)\n",
    "        targ.append(line.split('\\t')[1])\n",
    "        input_lang.index_words(src_element)\n",
    "        output_lang.index_words(line.split('\\t')[1])\n",
    "    ##\n",
    "    for line in test_src_f.read().splitlines():\n",
    "        src_element = line.split('\\t')[0].replace(\" \", \"\")\n",
    "        if len(src_element) > 20: \n",
    "          src_element = src_element[len(src_element)-20:len(src_element)]\n",
    "          src_element = list(src_element)\n",
    "          src_element = ' '.join(src_element)\n",
    "          test_src.append(src_element)\n",
    "        else:\n",
    "          src_element = list(src_element)\n",
    "          src_element = ' '.join(src_element)\n",
    "          test_src.append(src_element)\n",
    "        test_targ.append(line.split('\\t')[1])\n",
    "\n",
    "\n",
    "    for i in range(len(test_src)):\n",
    "        test_pair.append([test_src[i]]+[test_targ[i]])\n",
    "\n",
    "    result_f = open('./result.txt','w')\n",
    "    result = list()\n",
    "    for i in range(len(test_pair)):\n",
    "        output_words, decoder_attn = evaluate(test_pair[i][0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        result.append(output_sentence)\n",
    "        print('%d : %s' %(i,output_sentence))\n",
    "        result_f.write(output_sentence + '\\n')\n",
    "    result_f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5pAVUS-pD4Yc"
   },
   "outputs": [],
   "source": [
    "#get predict_seq from file\n",
    "result = list()\n",
    "f = open('result.txt')\n",
    "\n",
    "for tag in f.read().splitlines():\n",
    "  result.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "r6w8JlpqRW3P"
   },
   "outputs": [],
   "source": [
    "#창원\n",
    "binary_result = list()\n",
    "binary_result2 = list()\n",
    "\n",
    "for element in result:\n",
    "  if element == '1' or element == '2':\n",
    "    element = '1'\n",
    "  elif element == '3':\n",
    "    element = '0'\n",
    "  binary_result.append(element)\n",
    "  \n",
    "for element in result:\n",
    "  if element == '1':\n",
    "    element = '1'\n",
    "  elif element == '2' or element == '3':\n",
    "    element = '0'\n",
    "  binary_result2.append(element)\n",
    "  \n",
    "predict_seq = binary_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 783,
     "status": "ok",
     "timestamp": 1542263579486,
     "user": {
      "displayName": "박준우",
      "photoUrl": "",
      "userId": "09139914674254720430"
     },
     "user_tz": -540
    },
    "id": "Jdkf01F_xy0h",
    "outputId": "8ed575a1-223f-4dd0-dfcd-63d129060a40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6745562130177515, 0.6195652173913043, 0.7402597402597403, 0.8166666666666667)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(binary_result2)):\n",
    "  binary_result2[i] = int(binary_result2[i])\n",
    "print(get_score(binary_result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "U3fqxTHpJV3T"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 718,
     "status": "ok",
     "timestamp": 1542261094816,
     "user": {
      "displayName": "박준우",
      "photoUrl": "",
      "userId": "09139914674254720430"
     },
     "user_tz": -540
    },
    "id": "5FpJuehMJ4pL",
    "outputId": "0c32c604-56bc-445c-e4d2-68f93e571ff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.07692307692307693, 0), (0.12, 16), (0.078125, 19), (0.09090909090909091, 110), (0.10112359550561797, 59), (0.08823529411764706, 42), (0.06060606060606061, 92), (0.125, 30), (0.03333333333333333, 21), (0.030303030303030304, 30), (0.06896551724137931, 72), (0.058823529411764705, 0), (0.04081632653061224, 36), (0.05063291139240506, 43), (0.058823529411764705, 7), (0.07246376811594203, 37), (0.057971014492753624, 5), (0.06382978723404255, 30), (0.12, 13), (0.058823529411764705, 9), (0.10204081632653061, 30), (0.08620689655172414, 59), (0.11475409836065574, 48), (0.08571428571428572, 108), (0.1076923076923077, 82), (0.10344827586206896, 82), (0.1, 23), (0.0547945205479452, 46), (0.09090909090909091, 49), (0.06097560975609756, 53), (0.020833333333333332, 0), (0.17647058823529413, 17), (0.04938271604938271, 62), (0.07692307692307693, 68), (0.047058823529411764, 59), (0.05952380952380952, 16), (0.05747126436781609, 59), (0.1111111111111111, 48), (0.13333333333333333, 48), (0.06818181818181818, 53), (0.08333333333333333, 11), (0.14814814814814814, 84), (0.14893617021276595, 54), (0.1, 17), (0.08928571428571429, 68), (0.08333333333333333, 58), (0.07272727272727272, 37), (0.05263157894736842, 30), (0.05660377358490566, 30), (0.060240963855421686, 59), (0.125, 76), (0.07317073170731707, 7), (0.05, 64), (0.07954545454545454, 36), (0.07692307692307693, 30), (0.0784313725490196, 36), (0.05, 6), (0.09523809523809523, 11), (0.08620689655172414, 54), (0.08823529411764706, 61), (0.04938271604938271, 56), (0.054945054945054944, 68), (0.06382978723404255, 36), (0.05970149253731343, 19), (0.038461538461538464, 116), (0.08955223880597014, 120), (0.07272727272727272, 43), (0.10112359550561797, 84), (0.05747126436781609, 5), (0.0594059405940594, 5), (0.05, 43), (0.10526315789473684, 57), (0.0449438202247191, 80), (0.07462686567164178, 10), (0.04285714285714286, 80), (0.16129032258064516, 68), (0.13333333333333333, 47), (0.08823529411764706, 9), (0.06976744186046512, 53), (0.0967741935483871, 55), (0.07954545454545454, 61), (0.08695652173913043, 33), (0.07894736842105263, 84), (0.05, 0), (0.06666666666666667, 57), (0.047058823529411764, 30), (0.0, 30), (0.14814814814814814, 11), (0.23076923076923078, 62), (0.06557377049180328, 60), (0.045454545454545456, 92), (0.1, 68), (0.06521739130434782, 82), (0.038461538461538464, 79), (0.045454545454545456, 23), (0.07142857142857142, 9), (0.06172839506172839, 27), (0.06741573033707865, 43), (0.08571428571428572, 66), (0.09523809523809523, 31), (0.13636363636363635, 21), (0.08333333333333333, 48), (0.0547945205479452, 11), (0.09803921568627451, 11), (0.0784313725490196, 37), (0.10810810810810811, 0), (0.03389830508474576, 101), (0.08536585365853659, 19), (0.16666666666666666, 90), (0.09375, 44), (0.14285714285714285, 46), (0.06521739130434782, 43), (0.05714285714285714, 19), (0.041666666666666664, 18), (0.10909090909090909, 59), (0.125, 30), (0.04878048780487805, 5), (0.0625, 5), (0.07692307692307693, 19), (0.14285714285714285, 38), (0.07352941176470588, 83), (0.056179775280898875, 16), (0.2777777777777778, 90), (0.07692307692307693, 30), (0.07407407407407407, 47), (0.045454545454545456, 6), (0.0449438202247191, 86), (0.06666666666666667, 62), (0.046875, 0), (0.061224489795918366, 34), (0.07462686567164178, 92), (0.14285714285714285, 21), (0.07142857142857142, 19), (0.18181818181818182, 53), (0.1, 44), (0.047619047619047616, 3), (0.18518518518518517, 55), (0.07142857142857142, 18), (0.1111111111111111, 46), (0.0821917808219178, 61), (0.14814814814814814, 67), (0.16666666666666666, 48), (0.125, 46), (0.06382978723404255, 103), (0.13333333333333333, 61), (0.1111111111111111, 54), (0.12162162162162163, 59), (0.125, 7), (0.16216216216216217, 67), (0.13333333333333333, 54), (0.09090909090909091, 48), (0.0975609756097561, 58), (0.15625, 48), (0.16, 54), (0.09090909090909091, 102), (0.056179775280898875, 112), (0.07894736842105263, 48), (0.11538461538461539, 55), (0.1, 47), (0.1111111111111111, 46), (0.06896551724137931, 49), (0.04878048780487805, 21), (0.04838709677419355, 24), (0.09090909090909091, 87), (0.06896551724137931, 123), (0.06896551724137931, 19), (0.058823529411764705, 0), (0.06493506493506493, 19), (0.08, 79), (0.11764705882352941, 7), (0.06451612903225806, 75), (0.15384615384615385, 30), (0.06896551724137931, 68), (0.10526315789473684, 61), (0.125, 16), (0.17647058823529413, 80), (0.056074766355140186, 36), (0.08, 68), (0.13043478260869565, 5), (0.037383177570093455, 92), (0.08139534883720931, 59), (0.03614457831325301, 36), (0.07692307692307693, 31), (0.030303030303030304, 5), (0.09090909090909091, 11), (0.06451612903225806, 0), (0.11764705882352941, 30), (0.09523809523809523, 59), (0.09836065573770492, 48), (0.06060606060606061, 48), (0.047619047619047616, 62), (0.1111111111111111, 80), (0.0547945205479452, 10), (0.07142857142857142, 84), (0.0425531914893617, 11), (0.07407407407407407, 13), (0.0684931506849315, 50), (0.07058823529411765, 113), (0.058823529411764705, 2), (0.0975609756097561, 84), (0.0759493670886076, 84), (0.047619047619047616, 11), (0.1111111111111111, 36), (0.1, 36), (0.0625, 11), (0.05333333333333334, 50), (0.0684931506849315, 103), (0.05333333333333334, 2), (0.1, 11), (0.075, 55), (0.07142857142857142, 57), (0.07142857142857142, 30), (0.06666666666666667, 19), (0.13636363636363635, 50), (0.14705882352941177, 38), (0.11764705882352941, 31), (0.11538461538461539, 53), (0.09090909090909091, 19), (0.09302325581395349, 58), (0.047619047619047616, 2), (0.13636363636363635, 103), (0.10126582278481013, 45), (0.15789473684210525, 46), (0.05128205128205128, 89), (0.06779661016949153, 11), (0.09523809523809523, 106), (0.0707070707070707, 84), (0.047619047619047616, 72), (0.05813953488372093, 31), (0.04819277108433735, 30), (0.08771929824561403, 19), (0.06153846153846154, 30), (0.0967741935483871, 11), (0.08695652173913043, 19), (0.07317073170731707, 19), (0.05263157894736842, 30), (0.10256410256410256, 90), (0.09090909090909091, 118), (0.16666666666666666, 2), (0.1, 21), (0.13043478260869565, 46), (0.07575757575757576, 48), (0.09090909090909091, 30), (0.05813953488372093, 16), (0.047058823529411764, 2), (0.041237113402061855, 5), (0.08571428571428572, 19), (0.054945054945054944, 120), (0.0684931506849315, 47), (0.07936507936507936, 59), (0.07042253521126761, 113), (0.07936507936507936, 44), (0.049019607843137254, 9), (0.06172839506172839, 84), (0.07865168539325842, 90), (0.08695652173913043, 117), (0.07792207792207792, 0), (0.06329113924050633, 68), (0.05434782608695652, 92), (0.12, 19), (0.09090909090909091, 46), (0.10526315789473684, 54), (0.13043478260869565, 113), (0.08196721311475409, 58), (0.07142857142857142, 17), (0.1509433962264151, 62), (0.1, 19), (0.10526315789473684, 19), (0.06382978723404255, 24), (0.05660377358490566, 68), (0.06451612903225806, 5), (0.07142857142857142, 68), (0.033707865168539325, 5), (0.07042253521126761, 92), (0.07142857142857142, 30), (0.0975609756097561, 37), (0.12, 48), (0.09523809523809523, 59), (0.057971014492753624, 4), (0.06, 49), (0.14285714285714285, 24), (0.06382978723404255, 120), (0.07407407407407407, 32), (0.03125, 26), (0.08695652173913043, 48), (0.06060606060606061, 11), (0.07547169811320754, 37), (0.07142857142857142, 92), (0.030303030303030304, 61), (0.0967741935483871, 24), (0.10344827586206896, 92), (0.047058823529411764, 86), (0.1111111111111111, 42), (0.038461538461538464, 43), (0.038461538461538464, 7), (0.047619047619047616, 68), (0.05555555555555555, 30), (0.12987012987012986, 84), (0.09090909090909091, 37), (0.07228915662650602, 103)]\n"
     ]
    }
   ],
   "source": [
    "origin_grams = list()\n",
    "for sentence in origin_sentences:\n",
    "    n_grams = set()\n",
    "    for gram in range(len(sentence)-1):\n",
    "        n_grams.add(sentence[gram:gram+2])\n",
    "    origin_grams.append(n_grams)\n",
    "\n",
    "    \n",
    "generated_grams = list()\n",
    "for sentence in sentences:\n",
    "    n_grams = set()\n",
    "    for gram in range(len(sentence)-1):\n",
    "        n_grams.add(sentence[gram:gram+2])\n",
    "    generated_grams.append(n_grams)\n",
    "    \n",
    "similarity_list = list()\n",
    "for generated_gram in generated_grams:\n",
    "    num_sim = 0\n",
    "    count = 0\n",
    "    for origin_gram in origin_grams:\n",
    "        inter_set = generated_gram.intersection(origin_gram)\n",
    "        if len(inter_set) > num_sim:\n",
    "            num_sim = len(inter_set)\n",
    "            sim_loc = count\n",
    "        count += 1\n",
    "    similarity_list.append((float(num_sim)/len(generated_gram),sim_loc))\n",
    "print((similarity_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 628,
     "status": "ok",
     "timestamp": 1542117633082,
     "user": {
      "displayName": "박준우",
      "photoUrl": "",
      "userId": "09139914674254720430"
     },
     "user_tz": -540
    },
    "id": "UBWzEnZDJz3e",
    "outputId": "e95a8427-1f45-44fc-ab7d-2c6abf53f549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7120418848167539,\n",
       " 0.5964912280701754,\n",
       " 0.8831168831168831,\n",
       " 0.8166666666666667)"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list = list()\n",
    "\n",
    "for i in range(len(predict_blacklist)):\n",
    "  if predict_blacklist[i] == predict_seq[i]:\n",
    "    final_list.append(predict_blacklist[i])\n",
    "  else:\n",
    "    final_list.append(origin_tag[similarity_list[i][1]])\n",
    "print(len(final_list))\n",
    "get_score(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 696,
     "status": "ok",
     "timestamp": 1542263036080,
     "user": {
      "displayName": "박준우",
      "photoUrl": "",
      "userId": "09139914674254720430"
     },
     "user_tz": -540
    },
    "id": "A7o1WZg26KMS",
    "outputId": "6ab1b029-2774-4ba9-a1c6-313b3282d36d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0]\n",
      "(0.45106382978723414, 0.33544303797468356, 0.6883116883116883, 0.57)\n",
      "(0.40563380281690137, 0.2589928057553957, 0.935064935064935, 0.2966666666666667)\n"
     ]
    }
   ],
   "source": [
    "k= 0.07\n",
    "predict_sim = list()\n",
    "for i in range(len(sentences)):\n",
    "  if similarity_list[i][0] < k:\n",
    "    predict_sim.append(0)\n",
    "  else:\n",
    "    predict_sim.append(origin_tag[similarity_list[i][1]])\n",
    "\n",
    "print(get_score(predict_sim))\n",
    "print(get_score(predict_sim2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1542263632799,
     "user": {
      "displayName": "박준우",
      "photoUrl": "",
      "userId": "09139914674254720430"
     },
     "user_tz": -540
    },
    "id": "9PWC3kCTDH7Y",
    "outputId": "fd88a5ed-812c-455d-e910-55fc7dab60b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7337278106508877, 0.6739130434782609, 0.8051948051948052, 0.85)"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list = list()\n",
    "for i in range(len(predict_blacklist)):\n",
    "  if predict_blacklist[i] == predict_seq[i]:\n",
    "    final_list.append(predict_blacklist[i])\n",
    "  elif predict_blacklist[i] == predict_sim[i]:\n",
    "    final_list.append(predict_blacklist[i])\n",
    "  else:\n",
    "    final_list.append(predict_seq[i])\n",
    "#     final_list.append(origin_tag[similarity_list[i][1]])\n",
    "print(len(final_list))\n",
    "get_score(final_list)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "verification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
